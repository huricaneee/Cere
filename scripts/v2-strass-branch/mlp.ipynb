{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003485eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/neural/mlp.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from ..base_classifier import BaseClassifier\n",
    "\n",
    "class MLPClassifier(BaseClassifier):\n",
    "    \"\"\"Multi-Layer Perceptron classifier using PyTorch\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        super().__init__(config)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def _create_model(self, input_dim: int, n_classes: int) -> nn.Module:\n",
    "        \"\"\"Create MLP model\"\"\"\n",
    "        hidden_dims = self.config.get('hidden_dims', [512, 256, 128])\n",
    "        dropout_rate = self.config.get('dropout_rate', 0.3)\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(prev_dim, n_classes))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'MLPClassifier':\n",
    "        \"\"\"Train MLP classifier\"\"\"\n",
    "        # Preprocess data\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        y_encoded = self.label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        y_tensor = torch.LongTensor(y_encoded).to(self.device)\n",
    "        \n",
    "        # Create model\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        n_classes = len(np.unique(y_encoded))\n",
    "        self.model = self._create_model(input_dim, n_classes).to(self.device)\n",
    "        \n",
    "        # Training setup\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config.get('learning_rate', 0.001),\n",
    "            weight_decay=self.config.get('weight_decay', 1e-4)\n",
    "        )\n",
    "        \n",
    "        # Create data loader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.config.get('batch_size', 32),\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        n_epochs = self.config.get('n_epochs', 100)\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if epoch % 20 == 0:\n",
    "                avg_loss = total_loss / len(dataloader)\n",
    "                print(f\"Epoch {epoch}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "        return self.label_encoder.inverse_transform(predictions)\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Return prediction probabilities\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before making predictions\")\n",
    "            \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    def get_feature_importance(self) -> np.ndarray:\n",
    "        \"\"\"Return feature importance (gradient-based)\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise RuntimeError(\"Model must be fitted before getting feature importance\")\n",
    "            \n",
    "        # Use gradient-based feature importance\n",
    "        dummy_input = torch.randn(1, self.scaler.n_features_in_).to(self.device)\n",
    "        dummy_input.requires_grad_(True)\n",
    "        \n",
    "        self.model.eval()\n",
    "        output = self.model(dummy_input)\n",
    "        output.backward(torch.ones_like(output))\n",
    "        \n",
    "        importance = torch.abs(dummy_input.grad).mean(dim=0).cpu().numpy()\n",
    "        return importance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
